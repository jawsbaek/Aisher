# Aisher ë ˆí¬ì§€í† ë¦¬ ê°œì„ ì‚¬í•­ ë¶„ì„

**ë¶„ì„ ë‚ ì§œ**: 2025-11-25
**ë¶„ì„ ëŒ€ìƒ**: Aisher v0.1.0 (AI-powered error log analyzer)
**ì½”ë“œë² ì´ìŠ¤ ê·œëª¨**: 2,810 lines (ì†ŒìŠ¤) + 2,114 lines (í…ŒìŠ¤íŠ¸)

---

## ğŸ“Š í˜„ì¬ ìƒíƒœ ìš”ì•½

### í”„ë¡œì íŠ¸ ê°œìš”
AisherëŠ” SigNoz/ClickHouseì—ì„œ OpenTelemetry ë¡œê·¸ë¥¼ ê°€ì ¸ì™€ TOON í¬ë§·ìœ¼ë¡œ ìµœì í™”í•˜ê³ , LLMì„ í†µí•´ ê·¼ë³¸ ì›ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” AI ê¸°ë°˜ ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.

### ì „ì²´ í‰ê°€
**ë“±ê¸‰**: B+ (ì–‘í˜¸, ê°œì„  ì—¬ì§€ ìˆìŒ)

**ê°•ì **:
- âœ… ìš°ìˆ˜í•œ ì½”ë“œ í’ˆì§ˆ ë° ëª¨ë“ˆ êµ¬ì¡°
- âœ… í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ (20+ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤)
- âœ… í˜ì‹ ì ì¸ TOON í¬ë§· êµ¬í˜„ (40-60% í† í° ì ˆê°)
- âœ… ì¶©ì‹¤í•œ ê°œë°œ ë¬¸ì„œ

**ì£¼ìš” ì•½ì **:
- âŒ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ (v2 vs v3)
- âŒ í”„ë¡œë•ì…˜ ìš´ì˜ ê¸°ëŠ¥ ë¯¸ë¹„
- âŒ ëª¨ë‹ˆí„°ë§ ë¶€ì¬ (ì˜ì¡´ì„±ë§Œ ì„ ì–¸ë¨)
- âŒ ìºì‹± ë ˆì´ì–´ ì—†ìŒ (ë¹„ìš© ë‚­ë¹„)

---

## ğŸš¨ Critical Issues (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)

### 1. ìŠ¤í‚¤ë§ˆ ë²„ì „ ë¶ˆì¼ì¹˜ (ì¹˜ëª…ì )

**ì‹¬ê°ë„**: ğŸ”´ CRITICAL

**ë¬¸ì œì **:
- í”„ë¡œë•ì…˜ ì½”ë“œëŠ” `distributed_signoz_index_v3` + `signoz_error_index_v2` ì‚¬ìš©
- í†µí•© í…ŒìŠ¤íŠ¸ëŠ” `signoz_index_v2` (v2 ìŠ¤í‚¤ë§ˆ)ë§Œ ìƒì„±
- í…ŒìŠ¤íŠ¸ê°€ ì‹¤ì œ Golden Queryë¥¼ ê²€ì¦í•˜ì§€ ëª»í•¨

**ì˜í–¥ë°›ëŠ” ì½”ë“œ**:
```python
# src/aisher/repository.py:97-110
FROM {database}.distributed_signoz_index_v3 AS t
LEFT JOIN {database}.distributed_signoz_error_index_v2 AS e
    ON t.trace_id = e.error_id
WHERE t.ts_bucket_start >= ...  # v3 ì „ìš© í•„ë“œ
  AND t.has_error = true        # v3 ì „ìš© í•„ë“œ

# í•˜ì§€ë§Œ tests/docker/clickhouse/init.sqlì€ v2 ìŠ¤í‚¤ë§ˆë§Œ ìƒì„±
CREATE TABLE signoz_index_v2 (
    -- ts_bucket_start ì—†ìŒ
    -- has_error ì—†ìŒ
    -- signoz_error_index_v2 í…Œì´ë¸” ìì²´ê°€ ì—†ìŒ
)
```

**ìˆ˜ì • ë°©ë²•**:
1. `tests/docker/clickhouse/init.sql`ì„ v3 ìŠ¤í‚¤ë§ˆë¡œ ì—…ë°ì´íŠ¸
2. `signoz_error_index_v2` í…Œì´ë¸” ì¶”ê°€
3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— v3 í•„ë“œ í¬í•¨
4. í†µí•© í…ŒìŠ¤íŠ¸ ê²€ì¦ ë¡œì§ ì—…ë°ì´íŠ¸

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 4-6 ì‹œê°„

---

### 2. ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ë²„ê·¸

**ì‹¬ê°ë„**: ğŸŸ  HIGH

**ë¬¸ì œì **:
```python
# src/aisher/repository.py:288-289
async def close(self):
    if self._client:
        await self._client.close()
        # ğŸ› BUG: _clientê°€ Noneìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•ŠìŒ
        # close()ë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ë©´ ì´ë¯¸ ë‹«íŒ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë‹¤ì‹œ ë‹«ìœ¼ë ¤ ì‹œë„
```

**ìˆ˜ì • ë°©ë²•**:
```python
async def close(self):
    if self._client:
        await self._client.close()
        self._client = None  # ì¶”ê°€ í•„ìš”
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 10ë¶„

---

### 3. Prometheus ì˜ì¡´ì„± ë¯¸ì‚¬ìš©

**ì‹¬ê°ë„**: ğŸŸ  HIGH

**ë¬¸ì œì **:
- `prometheus-client==0.19.0`ì´ ì˜ì¡´ì„±ì— ì„ ì–¸ë˜ì–´ ìˆìŒ
- ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ì½”ë“œê°€ ì „í˜€ ì—†ìŒ
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ìš´ì˜ ê°€ì‹œì„± ì œë¡œ

**í•„ìš”í•œ ë©”íŠ¸ë¦­**:
```python
# êµ¬í˜„ í•„ìš”
from prometheus_client import Counter, Histogram, Gauge

errors_fetched_total = Counter(
    'aisher_errors_fetched_total',
    'Total errors fetched from ClickHouse',
    ['service_name']
)

llm_analysis_duration_seconds = Histogram(
    'aisher_llm_analysis_duration_seconds',
    'Time spent on LLM analysis',
    ['model', 'status']
)

clickhouse_query_errors_total = Counter(
    'aisher_clickhouse_errors_total',
    'ClickHouse query failures',
    ['error_type']
)

toon_format_compression_ratio = Histogram(
    'aisher_toon_compression_ratio',
    'TOON vs JSON size ratio'
)
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 1-2ì¼

---

## ğŸ’¡ ì£¼ìš” ê°œì„  ê¶Œì¥ì‚¬í•­

### P0 - Critical (ì¦‰ì‹œ ìˆ˜ì •)

#### 1. í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í‚¤ë§ˆ ìˆ˜ì •
**ëª©í‘œ**: í…ŒìŠ¤íŠ¸ê°€ ì‹¤ì œ í”„ë¡œë•ì…˜ ì¿¼ë¦¬ë¥¼ ê²€ì¦í•˜ë„ë¡ ë³´ì¥

**ì‘ì—… í•­ëª©**:
- [ ] `tests/docker/clickhouse/init.sql` v3 ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
- [ ] `signoz_error_index_v2` í…Œì´ë¸” ìƒì„±
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— `ts_bucket_start`, `has_error` ì¶”ê°€
- [ ] `trace_id` (ì†Œë¬¸ì) ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ assertion ì—…ë°ì´íŠ¸

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 4-6 ì‹œê°„

#### 2. ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ ìˆ˜ì •
**ëª©í‘œ**: ì¤‘ë³µ close() í˜¸ì¶œ ë° ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€

**ì‘ì—… í•­ëª©**:
- [ ] `repository.py:close()`ì—ì„œ `self._client = None` ì¶”ê°€
- [ ] close()ì— íƒ€ì„ì•„ì›ƒ ì¶”ê°€ (`asyncio.wait_for`)
- [ ] í…ŒìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ close() ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 30ë¶„

---

### P1 - High (ë‹¤ìŒ ìŠ¤í”„ë¦°íŠ¸)

#### 3. Prometheus ëª¨ë‹ˆí„°ë§ êµ¬í˜„
**ëª©í‘œ**: í”„ë¡œë•ì…˜ ìš´ì˜ ê°€ì‹œì„± í™•ë³´

**ì‘ì—… í•­ëª©**:
- [ ] ë©”íŠ¸ë¦­ ì •ì˜ ë° ìˆ˜ì§‘ (`src/aisher/metrics.py`)
- [ ] `/metrics` ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (FastAPI ë˜ëŠ” standalone)
- [ ] Grafana ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ ìƒì„±
- [ ] ì•Œë¦¼ ê·œì¹™ ì •ì˜ (Prometheus Alertmanager)

**ì˜ˆì œ êµ¬í˜„**:
```python
# src/aisher/metrics.py
from prometheus_client import start_http_server, Counter, Histogram
import time

errors_processed = Counter('aisher_errors_processed', 'Errors analyzed')
llm_duration = Histogram('aisher_llm_seconds', 'LLM analysis time')

# src/aisher/analyzer.pyì—ì„œ ì‚¬ìš©
async def analyze_batch(self, errors):
    start = time.time()
    try:
        result = await self._analyze_internal(errors)
        llm_duration.observe(time.time() - start)
        errors_processed.inc(len(errors))
        return result
    except Exception:
        # ì—ëŸ¬ ë©”íŠ¸ë¦­ ê¸°ë¡
        raise
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 2-3ì¼

#### 4. ë°°í¬ ì„¤ì • ì¶”ê°€
**ëª©í‘œ**: í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°

**ì‘ì—… í•­ëª©**:
- [ ] `Dockerfile` ìƒì„± (multi-stage build)
- [ ] `docker-compose.prod.yml` ì‘ì„±
- [ ] Kubernetes manifests (Deployment, Service, ConfigMap)
- [ ] Helm chart ìƒì„± (ì„ íƒ)
- [ ] Health check ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
- [ ] Graceful shutdown í•¸ë“¤ë§

**ì˜ˆì œ Dockerfile**:
```dockerfile
# Multi-stage build for smaller image
FROM python:3.12-slim as builder
WORKDIR /app
RUN pip install uv
COPY pyproject.toml uv.lock ./
RUN uv sync --no-dev

FROM python:3.12-slim
WORKDIR /app
COPY --from=builder /app/.venv /app/.venv
COPY src/ /app/src/
ENV PATH="/app/.venv/bin:$PATH"
EXPOSE 8080
CMD ["python", "-m", "aisher.main"]
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 3-4ì¼

#### 5. ìºì‹± ë ˆì´ì–´ êµ¬í˜„
**ëª©í‘œ**: LLM API ë¹„ìš© 60-80% ì ˆê°

**ì‘ì—… í•­ëª©**:
- [ ] Redis í´ë¼ì´ì–¸íŠ¸ í†µí•©
- [ ] ì—ëŸ¬ íŒ¨í„´ í•´ì‹± ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- [ ] ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ë©”íŠ¸ë¦­ ì¶”ê°€
- [ ] TTL ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”
- [ ] ì‹œë§¨í‹± ìœ ì‚¬ë„ ë§¤ì¹­ (ì„ íƒ)

**ì˜ˆì œ êµ¬í˜„**:
```python
# src/aisher/cache.py
import hashlib
import json
from redis.asyncio import Redis

class AnalysisCache:
    def __init__(self, redis_url: str, ttl: int = 3600):
        self.redis = Redis.from_url(redis_url)
        self.ttl = ttl

    def _hash_errors(self, errors: list[ErrorLog]) -> str:
        """ì—ëŸ¬ íŒ¨í„´ì˜ í•´ì‹œ ìƒì„±"""
        pattern = "|".join(sorted([
            f"{e.svc}:{e.op}:{e.msg}" for e in errors
        ]))
        return hashlib.sha256(pattern.encode()).hexdigest()

    async def get(self, errors: list[ErrorLog]) -> str | None:
        key = self._hash_errors(errors)
        result = await self.redis.get(f"analysis:{key}")
        if result:
            cache_hits.inc()
        else:
            cache_misses.inc()
        return result

    async def set(self, errors: list[ErrorLog], analysis: str):
        key = self._hash_errors(errors)
        await self.redis.setex(f"analysis:{key}", self.ttl, analysis)
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 2-3ì¼

---

### P2 - Medium (ë¡œë“œë§µ)

#### 6. REST API êµ¬ì¶•
**ëª©í‘œ**: ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì›¹ ì„œë¹„ìŠ¤ë¡œ ì „í™˜

**ì‘ì—… í•­ëª©**:
- [ ] FastAPI í”„ë ˆì„ì›Œí¬ í†µí•©
- [ ] `/analyze` POST ì—”ë“œí¬ì¸íŠ¸
- [ ] `/health` í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
- [ ] OpenAPI/Swagger ë¬¸ì„œ ìë™ ìƒì„±
- [ ] API ì¸ì¦ (JWT ë˜ëŠ” API Key)
- [ ] Rate limiting

**ì˜ˆì œ API**:
```python
# src/aisher/api.py
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel

app = FastAPI(title="Aisher API")

class AnalysisRequest(BaseModel):
    time_window_minutes: int = 60
    limit: int = 10
    services: list[str] | None = None

@app.post("/analyze")
async def analyze_errors(
    request: AnalysisRequest,
    api_key: str = Depends(verify_api_key)
):
    repo = SigNozRepository()
    try:
        errors = await repo.fetch_errors(
            time_window_minutes=request.time_window_minutes,
            limit=request.limit
        )

        # ìºì‹œ í™•ì¸
        cached = await cache.get(errors)
        if cached:
            return {"analysis": cached, "cached": True}

        # LLM ë¶„ì„
        analyzer = BatchAnalyzer()
        analysis = await analyzer.analyze_batch(errors)
        await cache.set(errors, analysis)

        return {"analysis": analysis, "cached": False}
    finally:
        await repo.close()

@app.get("/health")
async def health_check():
    # ClickHouse ì—°ê²° í…ŒìŠ¤íŠ¸
    # LLM API í‚¤ ê²€ì¦
    return {"status": "healthy"}
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 1ì£¼ì¼

#### 7. ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™”
**ëª©í‘œ**: ë‹¨ì¼ span ì´ìƒì˜ ì •ë³´ ì œê³µ

**ì‘ì—… í•­ëª©**:
- [ ] Parent span ì •ë³´ í¬í•¨
- [ ] ë™ì¼ trace_idì˜ ê´€ë ¨ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
- [ ] ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ê·¸ë˜í”„ ì¡°íšŒ
- [ ] ê³¼ê±° ìœ ì‚¬ ì—ëŸ¬ íŒ¨í„´ ì¡°íšŒ
- [ ] ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„ (CPU/ë©”ëª¨ë¦¬ ìŠ¤íŒŒì´í¬)

**ì˜ˆì œ ì¿¼ë¦¬**:
```sql
-- Parent span context
SELECT
    parent.name as parent_operation,
    parent.serviceName as parent_service,
    parent.durationNano / 1000000 as parent_duration_ms
FROM signoz_traces.distributed_signoz_index_v3 AS child
LEFT JOIN signoz_traces.distributed_signoz_index_v3 AS parent
    ON child.parent_span_id = parent.span_id
    AND child.trace_id = parent.trace_id
WHERE child.span_id = ?

-- Related logs
SELECT timestamp, severity_text, body
FROM signoz_logs.distributed_logs_v2
WHERE trace_id = ?
ORDER BY timestamp
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 1-2ì£¼

#### 8. CI/CD ê°œì„ 
**ëª©í‘œ**: í’ˆì§ˆ ê²Œì´íŠ¸ ë° ìë™í™” ê°•í™”

**ì‘ì—… í•­ëª©**:
- [ ] ìµœì†Œ ì»¤ë²„ë¦¬ì§€ ì„ê³„ê°’ 80% ì„¤ì •
- [ ] Linting ì‹¤íŒ¨ ì‹œ ë¹Œë“œ ì‹¤íŒ¨ (strict mode)
- [ ] ë³´ì•ˆ ìŠ¤ìºë‹ (Bandit, Safety)
- [ ] ì˜ì¡´ì„± ì·¨ì•½ì  ìŠ¤ìº” (Snyk, Dependabot)
- [ ] Docker ì´ë¯¸ì§€ ìë™ ë¹Œë“œ ë° í‘¸ì‹œ
- [ ] ì‹œë§¨í‹± ë²„ì €ë‹ ìë™í™”
- [ ] Changelog ìë™ ìƒì„±

**ì˜ˆì œ GitHub Actions**:
```yaml
# .github/workflows/security.yml
name: Security Scan

on: [push, pull_request]

jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Bandit
        run: |
          pip install bandit
          bandit -r src/ -f json -o bandit-report.json

      - name: Run Safety
        run: |
          pip install safety
          safety check --json

      - name: Dependency Review
        uses: actions/dependency-review-action@v3
        if: github.event_name == 'pull_request'
```

**ì˜ˆìƒ ì‘ì—…ëŸ‰**: 2-3ì¼

---

### P3 - Nice to Have (ì¥ê¸° ê³„íš)

#### 9. ê³ ê¸‰ ê¸°ëŠ¥
- [ ] ë©€í‹°í…Œë„ŒíŠ¸ ì§€ì›
- [ ] ì»¤ìŠ¤í…€ ë¶„ì„ í…œí”Œë¦¿/í”„ë¡¬í”„íŠ¸
- [ ] Alert routing (PagerDuty, Slack, Email)
- [ ] ë¹„ìš© ì¶”ì  ë° ì˜ˆì‚° ì œí•œ
- [ ] A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬
- [ ] ë°°ì¹˜ ì²˜ë¦¬ í (Celery/RQ)
- [ ] ìŠ¤ì¼€ì¤„ëœ ë¶„ì„ (cron jobs)

#### 10. ë¬¸ì„œí™” ê°œì„ 
- [ ] ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ (Mermaid/PlantUML)
- [ ] ë°°í¬ ê°€ì´ë“œ
- [ ] íŠ¸ëŸ¬ë¸”ìŠˆíŒ… í”Œë ˆì´ë¶
- [ ] ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ
- [ ] ë¹„ìš© ìµœì í™” ê°€ì´ë“œ
- [ ] Contributing ê°€ì´ë“œë¼ì¸
- [ ] ë¦´ë¦¬ìŠ¤ í”„ë¡œì„¸ìŠ¤ ë¬¸ì„œ

---

## ğŸ”§ ê¸°ìˆ  ë¶€ì±„ ë° ì½”ë“œ í’ˆì§ˆ

### Minor Issues

#### 1. íƒ€ì… íŒíŠ¸ ë¶ˆì¼ì¹˜
```python
# src/aisher/repository.py:256
def _truncate_stacktrace(self, stack: str, max_length: int):
    # ë°˜í™˜ íƒ€ì… ì—†ìŒ - str ì¶”ê°€ í•„ìš”
    ...

# src/aisher/analyzer.py:89
async def analyze_batch(self, errors):
    # ë°˜í™˜ íƒ€ì…ì´ dictì´ì§€ë§Œ ëª…ì‹œ ì•ˆ ë¨
    # -> dict[str, Any] ì¶”ê°€ í•„ìš”
```

#### 2. ë§¤ì§ ë„˜ë²„
```python
# src/aisher/repository.py:200
wait_time = 2 ** attempt  # ìƒìˆ˜ë¡œ ì¶”ì¶œ í•„ìš”

# ê°œì„ :
BACKOFF_BASE = 2
BACKOFF_MAX_SECONDS = 60

wait_time = min(BACKOFF_BASE ** attempt, BACKOFF_MAX_SECONDS)
```

#### 3. í•œê¸€ ì£¼ì„
```python
# src/aisher/models.py:22
# ì„œë¹„ìŠ¤ ì´ë¦„ (ì˜ˆ: "api-gateway")  # ì˜ë¬¸ìœ¼ë¡œ ë³€ê²½ í•„ìš”
# Service name (e.g., "api-gateway")
```

#### 4. ì—„ê²©í•œ ì˜ì¡´ì„± ë²„ì „ ê³ ì •
```toml
# pyproject.toml - í˜„ì¬
litellm = "==1.30.0"  # ë³´ì•ˆ íŒ¨ì¹˜ ì°¨ë‹¨ë¨

# ê¶Œì¥
litellm = ">=1.30.0,<2.0.0"  # ë§ˆì´ë„ˆ ì—…ë°ì´íŠ¸ í—ˆìš©
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ê¸°íšŒ

### 1. ì—°ê²° í’€ë§
**í˜„ì¬**: ë§¤ ì¿¼ë¦¬ë§ˆë‹¤ ìƒˆë¡œìš´ ì—°ê²° ìƒì„±
**ê°œì„ **: ClickHouse connection pool ì‚¬ìš©

```python
# src/aisher/repository.py
from clickhouse_connect.driver.asyncclient import AsyncClient

class SigNozRepository:
    _pool: ClassVar[AsyncClient | None] = None

    @classmethod
    async def get_pool(cls) -> AsyncClient:
        if cls._pool is None:
            cls._pool = await get_async_client(
                host=settings.CLICKHOUSE_HOST,
                port=settings.CLICKHOUSE_PORT,
                # ì—°ê²° í’€ ì„¤ì •
                pool_size=10,
                pool_timeout=30
            )
        return cls._pool
```

### 2. ì¿¼ë¦¬ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
**í˜„ì¬**: ì „ì²´ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ
**ê°œì„ **: ëŒ€ìš©ëŸ‰ ê²°ê³¼ì…‹ì— ëŒ€í•´ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©

```python
async def fetch_errors_streaming(self, limit: int = 100):
    """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬"""
    query = "..."
    async for row in self._client.query_stream(query):
        yield ErrorLog(
            id=row['id'],
            svc=row['svc'],
            # ...
        )
```

### 3. ë³‘ë ¬ LLM í˜¸ì¶œ
**í˜„ì¬**: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìˆœì°¨ ì²˜ë¦¬
**ê°œì„ **: ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬

```python
async def analyze_multiple_batches(self, error_batches: list[list[ErrorLog]]):
    """ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„"""
    tasks = [
        self.analyze_batch(batch)
        for batch in error_batches
    ]
    return await asyncio.gather(*tasks, return_exceptions=True)
```

---

## ğŸ”’ ë³´ì•ˆ ê°œì„ ì‚¬í•­

### 1. Secrets Management í†µí•©
```python
# src/aisher/config.py
from aws_secretsmanager import get_secret_value

class Settings(BaseSettings):
    # AWS Secrets Manager ì‚¬ìš©
    OPENAI_API_KEY: SecretStr = Field(
        default_factory=lambda: get_secret_value("prod/aisher/openai_key")
    )
```

### 2. TLS/SSL ì„¤ì •
```python
# ClickHouse HTTPS ì‚¬ìš©
CLICKHOUSE_PORT: int = 8443  # HTTP 8123 ëŒ€ì‹ 
CLICKHOUSE_SECURE: bool = True
CLICKHOUSE_VERIFY_SSL: bool = True
```

### 3. Stack Trace ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹
```python
import re

def mask_sensitive_data(stack: str) -> str:
    """ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ì—ì„œ ë¯¼ê° ì •ë³´ ì œê±°"""
    # IP ì£¼ì†Œ ë§ˆìŠ¤í‚¹
    stack = re.sub(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', '[IP]', stack)

    # API í‚¤ íŒ¨í„´ ë§ˆìŠ¤í‚¹
    stack = re.sub(r'sk-[a-zA-Z0-9]{48}', '[API_KEY]', stack)

    # íŒŒì¼ ê²½ë¡œ ë§ˆìŠ¤í‚¹
    stack = re.sub(r'/home/[^/]+/', '/home/[USER]/', stack)

    return stack
```

### 4. Audit Logging
```python
# src/aisher/audit.py
import structlog

audit_logger = structlog.get_logger("audit")

async def analyze_with_audit(user_id: str, errors: list[ErrorLog]):
    audit_logger.info(
        "analysis_started",
        user_id=user_id,
        error_count=len(errors),
        services=[e.svc for e in errors]
    )

    result = await analyzer.analyze_batch(errors)

    audit_logger.info(
        "analysis_completed",
        user_id=user_id,
        duration_seconds=duration
    )

    return result
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ê°œì„ 

### í˜„ì¬ ì»¤ë²„ë¦¬ì§€ ê°­

#### 1. E2E í…ŒìŠ¤íŠ¸ (ì‹¤ì œ LLM API)
```python
# tests/test_e2e.py
@pytest.mark.e2e
@pytest.mark.skipif(not os.getenv("OPENAI_API_KEY"), reason="No API key")
async def test_real_llm_analysis():
    """ì‹¤ì œ LLM APIë¥¼ ì‚¬ìš©í•œ ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸"""
    repo = SigNozRepository()
    analyzer = BatchAnalyzer()

    try:
        errors = await repo.fetch_errors(limit=5)
        analysis = await analyzer.analyze_batch(errors)

        # ì‘ë‹µ ê²€ì¦
        assert "root_cause" in analysis
        assert len(analysis["root_cause"]) > 0
    finally:
        await repo.close()
```

#### 2. ë¡œë“œ í…ŒìŠ¤íŠ¸
```python
# tests/test_performance.py
@pytest.mark.performance
async def test_large_dataset_performance():
    """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    errors = [create_mock_error() for _ in range(10000)]

    start = time.time()
    formatted = ToonFormatter.format_tabular(errors, "perf_test")
    duration = time.time() - start

    # 10K ì—ëŸ¬ë¥¼ 1ì´ˆ ì´ë‚´ì— í¬ë§·íŒ…
    assert duration < 1.0

    # ì••ì¶•ë¥  ê²€ì¦ (TOONì´ JSONë³´ë‹¤ 40% ì´ìƒ ì‘ì•„ì•¼ í•¨)
    json_size = len(json.dumps([e.dict() for e in errors]))
    toon_size = len(formatted)
    assert toon_size < json_size * 0.6
```

#### 3. Chaos Testing
```python
# tests/test_chaos.py
@pytest.mark.chaos
async def test_database_connection_failure():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤"""
    repo = SigNozRepository()

    # ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
    with patch.object(repo._client, 'query', side_effect=ConnectionError):
        errors = await repo.fetch_errors()

        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì˜ˆì™¸ ë°œìƒ ì•ˆ í•¨)
        assert errors == []

        # ì—ëŸ¬ ë¡œê·¸ í™•ì¸
        assert "ClickHouse connection failed" in caplog.text
```

---

## ğŸ“¦ ë°°í¬ ë° ìš´ì˜

### í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ë°°í¬ ì „ í•„ìˆ˜ì‚¬í•­
- [ ] í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ (`CLICKHOUSE_*`, `LLM_*`)
- [ ] ClickHouse ì—°ê²° í…ŒìŠ¤íŠ¸
- [ ] LLM API í‚¤ ê²€ì¦
- [ ] í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ë™ì‘ í™•ì¸
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™œì„±í™”
- [ ] ë¡œê·¸ ë ˆë²¨ ì„¤ì • (INFO ë˜ëŠ” WARNING)
- [ ] ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì • (CPU/ë©”ëª¨ë¦¬)
- [ ] íƒ€ì„ì•„ì›ƒ ì„¤ì • íŠœë‹

#### ëª¨ë‹ˆí„°ë§ ì„¤ì •
```yaml
# prometheus/alerts.yml
groups:
  - name: aisher
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(aisher_clickhouse_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High ClickHouse error rate"

      - alert: LLMTimeoutSpike
        expr: rate(aisher_llm_timeout_total[10m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "LLM API timeout spike"

      - alert: CacheMissRateHigh
        expr: |
          rate(aisher_cache_misses_total[5m])
          / rate(aisher_cache_requests_total[5m]) > 0.8
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Cache efficiency degraded"
```

#### Kubernetes Deployment
```yaml
# k8s/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aisher
spec:
  replicas: 3
  selector:
    matchLabels:
      app: aisher
  template:
    metadata:
      labels:
        app: aisher
    spec:
      containers:
      - name: aisher
        image: aisher:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        env:
        - name: CLICKHOUSE_HOST
          valueFrom:
            configMapKeyRef:
              name: aisher-config
              key: clickhouse.host
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: aisher-secrets
              key: openai.key
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

---

## ğŸ’° ë¹„ìš© ìµœì í™”

### LLM ë¹„ìš© ì ˆê° ì „ëµ

#### 1. ìºì‹±ìœ¼ë¡œ 60-80% ì ˆê°
```python
# ì˜ˆìƒ ì ˆê°ì•¡
# ê°€ì •: í•˜ë£¨ 1000ê±´ ë¶„ì„, GPT-4 Turbo $0.01/1K tokens
# í‰ê·  ìš”ì²­ 2K tokens (ì…ë ¥ 1K + ì¶œë ¥ 1K)

# ìºì‹± ì „:
daily_requests = 1000
avg_tokens_per_request = 2000
cost_per_1k_tokens = 0.01
daily_cost_before = (daily_requests * avg_tokens_per_request / 1000) * cost_per_1k_tokens
# = $20/day = $600/month

# ìºì‹± í›„ (70% íˆíŠ¸ìœ¨):
cache_hit_rate = 0.70
daily_cost_after = daily_cost_before * (1 - cache_hit_rate)
# = $6/day = $180/month
# ì ˆê°ì•¡: $420/month
```

#### 2. ëª¨ë¸ ì„ íƒ ìµœì í™”
```python
# src/aisher/analyzer.py
def select_model(error_count: int, complexity_score: float) -> str:
    """ì—ëŸ¬ ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""
    if complexity_score < 0.3 or error_count == 1:
        return "gpt-3.5-turbo"  # $0.001/1K tokens
    elif complexity_score < 0.7:
        return "gpt-4-turbo"    # $0.01/1K tokens
    else:
        return "gpt-4o"         # $0.005/1K tokens

def calculate_complexity(errors: list[ErrorLog]) -> float:
    """ì—ëŸ¬ ë³µì¡ë„ ê³„ì‚° (0-1)"""
    factors = {
        'unique_services': len(set(e.svc for e in errors)) / 10,
        'avg_stack_length': np.mean([len(e.stack) for e in errors]) / 1000,
        'error_diversity': len(set(e.msg for e in errors)) / len(errors)
    }
    return np.mean(list(factors.values()))
```

#### 3. í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```python
from litellm import completion_cost

async def analyze_with_cost_tracking(self, errors):
    response = await acompletion(...)

    cost = completion_cost(
        model=self.model,
        prompt_tokens=response.usage.prompt_tokens,
        completion_tokens=response.usage.completion_tokens
    )

    cost_tracker.observe(cost)
    logger.info(f"LLM cost: ${cost:.4f}")

    return response.choices[0].message.content
```

---

## ğŸ¯ ë¡œë“œë§µ ë° ìš°ì„ ìˆœìœ„

### Sprint 1 (1ì£¼) - Critical Fixes
- [x] í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í‚¤ë§ˆ v3 ì—…ë°ì´íŠ¸
- [x] ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ë²„ê·¸ ìˆ˜ì •
- [x] íƒ€ì… íŒíŠ¸ ë³´ì™„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¶”ê°€ (ëª©í‘œ: 90% ì»¤ë²„ë¦¬ì§€)

### Sprint 2 (2ì£¼) - ìš´ì˜ ê¸°ëŠ¥
- [ ] Prometheus ë©”íŠ¸ë¦­ êµ¬í˜„
- [ ] í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
- [ ] Dockerfile ë° docker-compose.prod.yml
- [ ] Kubernetes manifests

### Sprint 3 (2ì£¼) - ì„±ëŠ¥ ë° ë¹„ìš©
- [ ] Redis ìºì‹± ë ˆì´ì–´
- [ ] ì—°ê²° í’€ë§
- [ ] ë¹„ìš© ì¶”ì  ëŒ€ì‹œë³´ë“œ
- [ ] ë¡œë“œ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬

### Sprint 4 (2ì£¼) - API ë° ì¸ì¦
- [ ] FastAPI REST API
- [ ] JWT ì¸ì¦
- [ ] Rate limiting
- [ ] OpenAPI ë¬¸ì„œ

### ì¥ê¸° ê³„íš (Q2-Q3)
- [ ] ë©€í‹°í…Œë„ŒíŠ¸ ì§€ì›
- [ ] ê³ ê¸‰ ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ (parent spans, related logs)
- [ ] Alert routing (Slack, PagerDuty)
- [ ] ì»¤ìŠ¤í…€ ë¶„ì„ í…œí”Œë¦¿
- [ ] A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬

---

## ğŸ“Š ì˜ˆìƒ íˆ¬ì ëŒ€ë¹„ íš¨ê³¼

| íˆ¬ì í•­ëª© | ì˜ˆìƒ ê¸°ê°„ | í•µì‹¬ íš¨ê³¼ |
|----------|----------|----------|
| Critical Fixes | 1ì£¼ | í…ŒìŠ¤íŠ¸ ì‹ ë¢°ì„± 100% í™•ë³´ |
| ëª¨ë‹ˆí„°ë§ êµ¬í˜„ | 2ì£¼ | í”„ë¡œë•ì…˜ ê°€ì‹œì„±, ì¥ì•  ëŒ€ì‘ ì‹œê°„ 80% ë‹¨ì¶• |
| ìºì‹± ë ˆì´ì–´ | 2ì£¼ | LLM ë¹„ìš© 70% ì ˆê° ($420/month) |
| REST API | 2ì£¼ | ì‚¬ìš©ì ê²½í—˜ ê°œì„ , í†µí•© ìš©ì´ì„± |
| ì „ì²´ (Phase 1) | 7ì£¼ | í”„ë¡œë•ì…˜ ë°°í¬ ê°€ëŠ¥ ìƒíƒœ ë‹¬ì„± |

---

## ğŸ ê²°ë¡ 

### í˜„ì¬ ìƒíƒœ
AisherëŠ” **íƒ„íƒ„í•œ ê¸°ìˆ ì  ê¸°ë°˜**ì„ ê°–ì¶˜ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. TOON í¬ë§· í˜ì‹ , í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸, ìš°ìˆ˜í•œ ì½”ë“œ í’ˆì§ˆì´ ê°•ì ì…ë‹ˆë‹¤.

### ì£¼ìš” ì œì•½ì‚¬í•­
1. **í”„ë¡œë•ì…˜ ìš´ì˜ ì¤€ë¹„ë„ ë¶€ì¡±** - ëª¨ë‹ˆí„°ë§, ë°°í¬ ì„¤ì •, ìºì‹± ë¯¸ë¹„
2. **í†µí•© í…ŒìŠ¤íŠ¸ ì‹ ë¢°ì„± ë¬¸ì œ** - ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ë¡œ ì‹¤ì œ ì¿¼ë¦¬ ë¯¸ê²€ì¦
3. **ë¹„ìš© íš¨ìœ¨ì„±** - ìºì‹± ì—†ì´ ëª¨ë“  ë¶„ì„ì—ì„œ LLM API í˜¸ì¶œ

### ê¶Œì¥ ì ‘ê·¼ë²•
**Phase 1 (7ì£¼)**: Critical + High ìš°ì„ ìˆœìœ„ í•­ëª©ì— ì§‘ì¤‘
- Week 1: Critical fixes
- Week 2-3: ëª¨ë‹ˆí„°ë§ ë° ë°°í¬
- Week 4-5: ìºì‹± ë ˆì´ì–´
- Week 6-7: REST API

ì´ ë‹¨ê³„ë¥¼ ì™„ë£Œí•˜ë©´ **í”„ë¡œë•ì…˜ í™˜ê²½ì— ìì‹  ìˆê²Œ ë°°í¬ ê°€ëŠ¥**í•©ë‹ˆë‹¤.

**Phase 2 (Q2)**: ê³ ê¸‰ ê¸°ëŠ¥ ë° í™•ì¥ì„±
- ë©€í‹°í…Œë„ŒíŠ¸, ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸, Alert routing

### ìµœì¢… í‰ê°€
7ì£¼ê°„ì˜ ì§‘ì¤‘ ê°œë°œë¡œ Aisherë¥¼ **ì‹¤í—˜ì  í”„ë¡œì íŠ¸**ì—ì„œ **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì†”ë£¨ì…˜**ìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**íˆ¬ì ê°€ì¹˜**: â­â­â­â­â˜† (4/5)
- ê¸°ìˆ ì  ê¸°ë°˜: í›Œë¥­í•¨
- í”„ë¡œë•ì…˜ ì¤€ë¹„ë„: ë³´í†µ
- ê°œì„  ì ì¬ë ¥: ë§¤ìš° ë†’ìŒ

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ì‘ì„±ì**: AI Code Analyzer
**ë‹¤ìŒ ë¦¬ë·° ì˜ˆì •**: 2025-12-25 (1ê°œì›” í›„)
